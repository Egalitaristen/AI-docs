# ==============================================================================
# README
# ==============================================================================
#
# # Comprehensive AI API Client Examples (Python)
#
# ## Purpose
# This script serves as a collection of Python code examples demonstrating how to interact
# with various popular AI APIs for common tasks. It is intended for:
#   - Developers seeking quickstart code snippets for different AI services.
#   - AI models that need a reference document to generate API client code.
#
# ## Features Covered
# - Large Language Model (LLM) Chat Completions (Text-based)
# - Multimodal Chat Completions (Text + Vision Input)
# - Image Generation (Text-to-Image)
# - Text-to-Speech (TTS) Synthesis
# - Speech-to-Text (STT) Transcription
#
# ## Providers Covered
# - OpenAI (GPT models, DALL-E, Whisper, TTS)
# - Anthropic (Claude models)
# - Google Cloud (Vertex AI Gemini, Imagen, Cloud TTS, Cloud Speech-to-Text)
# - Google AI (Gemini API via AI Studio key)
# - Mistral AI (Mistral models)
# - Groq (Llama, Mixtral, Whisper, PlayAI TTS, Llama Vision models via GroqCloud)
# - Stability AI (Stable Diffusion models)
# - Local Servers (Examples using clients for LM Studio & Ollama, primarily text)
#
# ## How to Use
# 1.  **Installation:** Install required libraries:
#     ```bash
#     pip install openai anthropic google-cloud-aiplatform google-generativeai mistralai groq ollama google-cloud-texttospeech google-cloud-speech stability-sdk Pillow requests
#     ```
#     (Consider using a `requirements.txt` or `pyproject.toml` for better dependency management).
# 2.  **API Keys & Authentication:**
#     - Obtain API keys from the respective provider websites (OpenAI, Anthropic, Google AI Studio, Mistral, Groq, Stability AI).
#     - Set these keys as environment variables (e.g., `OPENAI_API_KEY`). This script will attempt to read them.
#     - For Google Cloud services (Vertex AI, Cloud TTS, STT), authenticate your environment, typically via `gcloud auth application-default login`. Update `gcp_project_id` if not using the default environment setting.
# 3.  **Local Servers (Optional):**
#     - If testing LM Studio or Ollama, ensure the respective application/service is running locally and the desired models are loaded/pulled. Update `lm_studio_base_url` or `ollama_host` if not using defaults.
# 4.  **Input Files:** Create placeholder or real `sample_audio.wav` and `sample_image.jpg` files in the same directory as the script, or update the paths in the configuration section. Dummy files will be created if they don't exist, but real files are needed for meaningful results.
# 5.  **Run:** Execute the script (`python your_script_name.py`). Uncomment the specific API calls you wish to test in the `if __name__ == "__main__":` block. Check the console output and the `api_outputs` directory.
#
# ## Disclaimer
# - This code is illustrative and intended for educational purposes.
# - **API Keys:** Handle your API keys securely. NEVER commit them directly into version control. Use environment variables or secure secret management solutions.
# - **Costs:** Be aware that making calls to these APIs incurs costs based on the provider's pricing model. Run examples judiciously.
# - **Error Handling:** The error handling is basic. Production applications require more robust error checking, retries, and logging.
# - **API Changes:** APIs evolve. While based on recent documentation, specific parameters or behaviors might change. Always refer to the official provider documentation.
#
# ==============================================================================

# Comprehensive LLM & MULTIMODAL API Client Examples
# Version 3.1: Added README and Suggested Improvements sections
# Covers: OpenAI, Anthropic, Google (Vertex AI / Cloud AI), Mistral, Groq, LM Studio, Ollama, Stability AI
# Features: Chat (Text & Vision), Image Generation, Text-to-Speech (TTS), Speech-to-Text (STT)

# --- Prerequisites ---
# (See README section above)
# ---

import os
import json
import io
import base64
import warnings
from PIL import Image
import requests
import time # For potential delays if needed

# Import specific clients/libraries
from openai import OpenAI as OpenAIClient # Renamed for clarity
import anthropic
from anthropic.types import ImageBlockParam
import vertexai
from vertexai.generative_models import GenerativeModel, Part, Image as VertexImage
from vertexai.preview.vision_models import ImageGenerationModel
from google.cloud import texttospeech as google_tts
from google.cloud import speech as google_stt
import google.generativeai as genai
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage as MistralChatMessage
from groq import Groq as GroqClient # Import Groq client
import ollama
import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation
from stability_sdk import client as stability_client


# --- Configuration & Placeholders ---
# Load keys from environment variables
openai_api_key = os.getenv("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY_PLACEHOLDER")
anthropic_api_key = os.getenv("ANTHROPIC_API_KEY", "YOUR_ANTHROPIC_API_KEY_PLACEHOLDER")
google_ai_api_key = os.getenv("GOOGLE_API_KEY", "YOUR_GOOGLE_AI_API_KEY_PLACEHOLDER")
mistral_api_key = os.getenv("MISTRAL_API_KEY", "YOUR_MISTRAL_API_KEY_PLACEHOLDER")
groq_api_key = os.getenv("GROQ_API_KEY", "YOUR_GROQ_API_KEY_PLACEHOLDER")
stability_api_key = os.getenv("STABILITY_API_KEY", "YOUR_STABILITY_API_KEY_PLACEHOLDER")

# Google Cloud Specific
gcp_project_id = os.getenv("GOOGLE_CLOUD_PROJECT", "your-gcp-project-id")
gcp_location = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")

# Local Server Endpoints
lm_studio_base_url = "http://localhost:1234/v1"
ollama_host = None # Uses default http://localhost:11434 if None

# File Paths (Create or replace with actual paths)
output_dir = "api_outputs"
sample_audio_path_for_stt = "sample_audio.wav" # CREATE THIS FILE for STT tests
sample_image_path_for_vision = "sample_image.jpg" # CREATE THIS FILE for Vision tests
sample_image_url_for_vision = "https://upload.wikimedia.org/wikipedia/commons/d/da/SF_From_Marin_Highlands3.jpg" # Example URL

# Ensure output directory exists
os.makedirs(output_dir, exist_ok=True)

# --- Helper Functions ---
def save_binary_data(data, filename):
    filepath = os.path.join(output_dir, filename)
    try:
        with open(filepath, "wb") as f: f.write(data)
        print(f"   [+] Saved output to: {filepath}")
    except Exception as e: print(f"   [!] Failed to save file {filepath}: {e}")

def encode_image_to_base64(image_path):
    if not os.path.exists(image_path): return None
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except Exception as e: print(f"   [!] Failed to encode image {image_path}: {e}"); return None

def load_image_bytes(image_path):
     if not os.path.exists(image_path): return None
     try:
         with open(image_path, "rb") as image_file: return image_file.read()
     except Exception as e: print(f"   [!] Failed to load image bytes {image_path}: {e}"); return None

# ==============================================================================
# SECTION 1: CHAT COMPLETIONS / TEXT & VISION
# ==============================================================================

# --- OpenAI Chat (Text & Vision) ---
def call_openai_chat(prompt: str, image_url: str = None, image_path: str = None, model_name: str = "gpt-4o"):
    print(f"\n--- Calling OpenAI Chat (Model: {model_name}) {'(Vision enabled)' if image_url or image_path else ''} ---")
    if openai_api_key.startswith("YOUR_"): print("   [!] OpenAI API key not set. Skipping."); return
    messages=[{"role": "system", "content": "You are a helpful assistant."}]
    user_content=[{"type": "text", "text": prompt}]
    if image_url: user_content.append({"type": "image_url", "image_url": {"url": image_url}}); print(f"   [i] Using image URL: {image_url}")
    elif image_path:
        if os.path.exists(image_path):
            base64_image = encode_image_to_base64(image_path)
            if base64_image:
                mime_type = "image/jpeg" if image_path.lower().endswith((".jpg", ".jpeg")) else "image/png"
                user_content.append({"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}})
                print(f"   [i] Using image path: {image_path} (encoded)")
            else: print(f"   [!] Failed to encode image: {image_path}. Skipping image.")
        else: print(f"   [!] Image file not found: {image_path}. Skipping image.");
    messages.append({"role": "user", "content": user_content})
    try:
        client = OpenAIClient(api_key=openai_api_key)
        response = client.chat.completions.create(model=model_name, messages=messages, temperature=0.7, max_tokens=500)
        if response.choices: print("   [->] Response:", response.choices[0].message.content); print("   [i] Usage:", response.usage)
        else: print("   [!] OpenAI API returned no choices.")
    except Exception as e: print(f"   [!] Error (OpenAI Chat): {e}")

# --- Anthropic Chat (Text & Vision) ---
def call_anthropic_chat(prompt: str, image_path: str = None, model_name: str = "claude-3-opus-20240229"):
    print(f"\n--- Calling Anthropic Chat (Model: {model_name}) {'(Vision enabled)' if image_path else ''} ---")
    if anthropic_api_key.startswith("YOUR_"): print("   [!] Anthropic API key not set. Skipping."); return
    user_content=[{"type": "text", "text": prompt}]
    if image_path:
        if os.path.exists(image_path):
            base64_image = encode_image_to_base64(image_path)
            if base64_image:
                media_type = "image/jpeg" if image_path.lower().endswith((".jpg", ".jpeg")) else "image/png"
                user_content.insert(0, ImageBlockParam(type="image", source={"type": "base64", "media_type": media_type, "data": base64_image}))
                print(f"   [i] Using image path: {image_path} (encoded)")
            else: print(f"   [!] Failed to encode image: {image_path}. Skipping image.")
        else: print(f"   [!] Image file not found: {image_path}. Skipping image.");
    messages=[{"role": "user", "content": user_content}]
    try:
        client = anthropic.Anthropic(api_key=anthropic_api_key)
        message = client.messages.create(model=model_name, max_tokens=500, temperature=0.7, system="You are a helpful assistant.", messages=messages)
        if message.content:
            text_response = "".join(block.text for block in message.content if block.type == 'text')
            print("   [->] Response:", text_response); print("   [i] Usage:", message.usage)
        else: print("   [!] Anthropic API returned no content.")
    except Exception as e: print(f"   [!] Error (Anthropic Chat): {e}")

# --- Google Vertex AI Chat (Text & Vision - Gemini) ---
def call_vertex_ai_chat(prompt: str, image_path: str = None, image_gcs_uri: str = None, model_name: str = "gemini-1.5-flash-001"):
    print(f"\n--- Calling Google Vertex AI Chat (Model: {model_name}) {'(Vision enabled)' if image_path or image_gcs_uri else ''} ---")
    if gcp_project_id == "your-gcp-project-id": print("   [!] GCP Project ID not set. Skipping."); return
    content_parts = [Part.from_text(prompt)]
    if image_gcs_uri:
         print(f"   [i] Using image GCS URI: {image_gcs_uri}")
         image_part = Part.from_uri(uri=image_gcs_uri, mime_type="image/jpeg") # Adjust mime_type if needed
         content_parts.insert(0, image_part)
    elif image_path:
         if os.path.exists(image_path):
            image_bytes = load_image_bytes(image_path)
            if image_bytes:
                mime_type = "image/jpeg" if image_path.lower().endswith((".jpg", ".jpeg")) else "image/png"
                image_part = Part.from_data(data=image_bytes, mime_type=mime_type)
                content_parts.insert(0, image_part); print(f"   [i] Using image path: {image_path}")
            else: print(f"   [!] Failed to load image bytes: {image_path}. Skipping image.")
         else: print(f"   [!] Image file not found: {image_path}. Skipping image.");
    try:
        vertexai.init(project=gcp_project_id, location=gcp_location)
        model = GenerativeModel(model_name)
        response = model.generate_content(content_parts, generation_config={"temperature": 0.7, "max_output_tokens": 500})
        if response.candidates: print("   [->] Response:", response.text); print("   [i] Usage:", getattr(response, 'usage_metadata', 'N/A'))
        else: print("   [!] Vertex AI API returned no candidates. Feedback:", getattr(response, 'prompt_feedback', 'N/A'))
    except Exception as e: print(f"   [!] Error (Vertex AI Chat): {e}. Ensure GCP auth configured.")

# --- Google AI Gemini Chat (Text & Vision) ---
def call_google_ai_chat(prompt: str, image_path: str = None, model_name: str = "gemini-1.5-flash"):
    print(f"\n--- Calling Google AI / Gemini Chat (Model: {model_name}) {'(Vision enabled)' if image_path else ''} ---")
    if google_ai_api_key.startswith("YOUR_"): print("   [!] Google AI (Gemini) API key not set. Skipping."); return
    content_parts = [prompt]
    if image_path:
        if os.path.exists(image_path):
            try: img = Image.open(image_path); content_parts.insert(0, img); print(f"   [i] Using image path: {image_path}")
            except Exception as e: print(f"   [!] Failed to open image {image_path}: {e}. Skipping image.")
        else: print(f"   [!] Image file not found: {image_path}. Skipping image.");
    try:
        genai.configure(api_key=google_ai_api_key)
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(content_parts, generation_config=genai.types.GenerationConfig(temperature=0.7, max_output_tokens=500))
        if response.parts: print("   [->] Response:", response.text)
        else: print("   [!] Google AI API returned no parts. Feedback:", getattr(response, 'prompt_feedback', 'N/A'))
    except Exception as e: print(f"   [!] Error (Google AI Chat): {e}")

# --- Groq Chat (Text & Vision - Llama 3.2) ---
def call_groq_chat(prompt: str, image_url: str = None, image_path: str = None, model_name: str = "llama3-8b-8192"):
    is_vision_model = "vision" in model_name.lower()
    print(f"\n--- Calling Groq Chat (Model: {model_name}) {'(Vision enabled)' if is_vision_model and (image_url or image_path) else ''} ---")
    if groq_api_key.startswith("YOUR_"): print("   [!] Groq API key not set. Skipping."); return
    messages=[]; user_content=[{"type": "text", "text": prompt}]
    if is_vision_model and (image_url or image_path):
        if image_url: user_content.append({"type": "image_url", "image_url": {"url": image_url}}); print(f"   [i] Using image URL: {image_url}")
        elif image_path:
            if os.path.exists(image_path):
                base64_image = encode_image_to_base64(image_path)
                if base64_image:
                    mime_type = "image/jpeg" if image_path.lower().endswith((".jpg", ".jpeg")) else "image/png"
                    user_content.append({"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}})
                    print(f"   [i] Using image path: {image_path} (encoded)")
                else: print(f"   [!] Failed to encode image: {image_path}. Skipping image.")
            else: print(f"   [!] Image file not found: {image_path}. Skipping image.")
        messages.append({"role": "user", "content": user_content})
    elif is_vision_model: messages.append({"role": "user", "content": prompt}) # Vision model, text only call
    else: messages.append({"role": "user", "content": prompt}) # Text model call
    try:
        client = GroqClient(api_key=groq_api_key)
        max_tokens_to_use = 1024 if is_vision_model else 150
        response = client.chat.completions.create(model=model_name, messages=messages, temperature=0.7, max_tokens=max_tokens_to_use)
        if response.choices: print("   [->] Response:", response.choices[0].message.content); print("   [i] Usage:", response.usage)
        else: print("   [!] Groq API returned no choices.")
    except Exception as e: print(f"   [!] Error (Groq Chat): {e}")

# --- Mistral Chat (Text Only via Standard Client) ---
def call_mistral_chat(prompt: str, model_name: str = "mistral-small-latest"):
    print(f"\n--- Calling Mistral AI Chat (Model: {model_name}) ---")
    if mistral_api_key.startswith("YOUR_"): print("   [!] Mistral API key not set. Skipping."); return
    try:
        client = MistralClient(api_key=mistral_api_key)
        response = client.chat(model=model_name, messages=[MistralChatMessage(role="user", content=prompt)], temperature=0.7, max_tokens=150)
        if response.choices: print("   [->] Response:", response.choices[0].message.content); print("   [i] Usage:", response.usage)
        else: print("   [!] Mistral API returned no choices.")
    except Exception as e: print(f"   [!] Error (Mistral Chat): {e}")

# --- LM Studio Chat (Text Primarily via Standard OpenAI Client) ---
def call_lm_studio_chat(prompt: str, model_name: str = "local-model"):
    print(f"\n--- Calling LM Studio Chat (Model: {model_name} / Loaded in UI) ---")
    try:
        client = OpenAIClient(base_url=lm_studio_base_url, api_key="lm-studio")
        response = client.chat.completions.create(model=model_name, messages=[{"role": "user", "content": prompt}], temperature=0.7, max_tokens=150)
        if response.choices: print("   [->] Response:", response.choices[0].message.content); print("   [i] Usage:", response.usage)
        else: print("   [!] LM Studio server returned no choices.")
    except Exception as e: print(f"   [!] Error (LM Studio Chat): {e}. Ensure server running.")

# --- Ollama Chat (Text Primarily via Standard Client, Vision needs specific models/API calls) ---
def call_ollama_chat(prompt: str, model_name: str = "llama3"):
    print(f"\n--- Calling Ollama Chat (Model: {model_name}) ---")
    client_args = {'host': ollama_host} if ollama_host else {}
    try:
        try: ollama.show(model_name, **client_args)
        except ollama.ResponseError as re: print(f"   [!] Ollama model '{model_name}' not found. Skipping."); return
        response = ollama.chat(model=model_name, messages=[{'role': 'user', 'content': prompt}], options={'temperature': 0.7, 'num_predict': 150}, **client_args)
        if response and 'message' in response and 'content' in response['message']:
            print("   [->] Response:", response['message']['content'])
            details = {k: v for k, v in response.items() if k not in ['message', 'model', 'created_at']}
            if details: print(f"   [i] Details: {json.dumps(details)}")
        else: print("   [!] Ollama API did not return expected message content.")
    except Exception as e: print(f"   [!] Error (Ollama Chat): {e}. Ensure service running.")

# ==============================================================================
# SECTION 2: IMAGE GENERATION
# ==============================================================================
# --- OpenAI Image Generation (DALL·E) ---
def call_openai_image_gen(prompt: str, model_name: str = "dall-e-3", n: int = 1, size: str = "1024x1024"):
    print(f"\n--- Calling OpenAI Image Generation (Model: {model_name}) ---")
    if openai_api_key.startswith("YOUR_"): print("   [!] OpenAI API key not set. Skipping."); return
    try:
        client = OpenAIClient(api_key=openai_api_key)
        response = client.images.generate(model=model_name, prompt=prompt, n=n, size=size, response_format="url")
        if response.data:
            for i, img_data in enumerate(response.data): print(f"   [->] Image {i+1} URL: {img_data.url}")
        else: print("   [!] OpenAI Image API returned no data.")
    except Exception as e: print(f"   [!] Error (OpenAI Image Gen): {e}")

# --- Google Vertex AI Image Generation (Imagen) ---
def call_vertex_ai_image_gen(prompt: str, model_id: str = "imagegeneration@006", n: int = 1):
    print(f"\n--- Calling Google Vertex AI Image Generation (Model: {model_id}) ---")
    if gcp_project_id == "your-gcp-project-id": print("   [!] GCP Project ID not set. Skipping."); return
    try:
        vertexai.init(project=gcp_project_id, location=gcp_location)
        model = ImageGenerationModel.from_pretrained(model_id)
        images = model.generate_images(prompt=prompt, number_of_images=n)
        if images:
            for i, image in enumerate(images):
                filename = f"vertex_imagen_{i+1}.png"; save_path = os.path.join(output_dir, filename)
                image.save(location=save_path, include_generation_parameters=True)
                print(f"   [+] Saved generated image to: {save_path}")
        else: print("   [!] Vertex AI Image API returned no images.")
    except Exception as e: print(f"   [!] Error (Vertex AI Image Gen): {e}. Ensure GCP auth and API enabled.")

# --- Stability AI Image Generation (Stable Diffusion) ---
def call_stability_ai_image_gen(prompt: str, engine_id: str = "stable-diffusion-xl-1024-v1-0", width: int = 1024, height: int = 1024, samples: int = 1):
    print(f"\n--- Calling Stability AI Image Generation (Engine: {engine_id}) ---")
    if stability_api_key.startswith("YOUR_"): print("   [!] Stability API key not set. Skipping."); return
    try:
        stability_connection = stability_client.StabilityInference(key=stability_api_key, verbose=False, engine=engine_id)
        answers = stability_connection.generate(prompt=prompt, height=height, width=width, samples=samples)
        img_saved = False
        for resp in answers:
            for artifact in resp.artifacts:
                if artifact.finish_reason == generation.FILTER: warnings.warn("   [!] Stability AI image filtered.")
                if artifact.type == generation.ARTIFACT_IMAGE:
                    img = Image.open(io.BytesIO(artifact.binary)); filename = f"stability_ai_{artifact.seed}.png"
                    save_path = os.path.join(output_dir, filename); img.save(save_path)
                    print(f"   [+] Saved generated image to: {save_path}"); img_saved = True
        if not img_saved: print("   [!] No image artifact received from Stability AI.")
    except Exception as e: print(f"   [!] Error (Stability AI Image Gen): {e}")

# ==============================================================================
# SECTION 3: TEXT-TO-SPEECH (TTS)
# ==============================================================================
# --- OpenAI TTS ---
def call_openai_tts(text_to_speak: str, model_name: str = "tts-1", voice: str = "alloy", output_filename: str = "openai_tts.mp3"):
    print(f"\n--- Calling OpenAI TTS (Model: {model_name}, Voice: {voice}) ---")
    if openai_api_key.startswith("YOUR_"): print("   [!] OpenAI API key not set. Skipping."); return
    try:
        client = OpenAIClient(api_key=openai_api_key)
        response = client.audio.speech.create(model=model_name, voice=voice, input=text_to_speak, response_format="mp3")
        save_binary_data(response.content, output_filename)
    except Exception as e: print(f"   [!] Error (OpenAI TTS): {e}")

# --- Google Cloud TTS ---
def call_google_cloud_tts(text_to_speak: str, language_code: str = "en-US", voice_name: str = "en-US-Standard-C", output_filename: str = "google_tts.mp3"):
    print(f"\n--- Calling Google Cloud TTS (Voice: {voice_name}) ---")
    if gcp_project_id == "your-gcp-project-id": print("   [!] GCP Project ID not set. Skipping."); return
    try:
        client = google_tts.TextToSpeechClient()
        synthesis_input = google_tts.SynthesisInput(text=text_to_speak)
        voice = google_tts.VoiceSelectionParams(language_code=language_code, name=voice_name)
        audio_config = google_tts.AudioConfig(audio_encoding=google_tts.AudioEncoding.MP3)
        response = client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)
        save_binary_data(response.audio_content, output_filename)
    except Exception as e: print(f"   [!] Error (Google Cloud TTS): {e}. Ensure GCP auth and API enabled.")

# --- Groq TTS ---
def call_groq_tts(text_to_speak: str, model_name: str = "playai-tts", voice: str = "Fritz-PlayAI", output_filename: str = "groq_tts.wav"):
    print(f"\n--- Calling Groq TTS (Model: {model_name}, Voice: {voice}) ---")
    if groq_api_key.startswith("YOUR_"): print("   [!] Groq API key not set. Skipping."); return
    try:
        client = GroqClient(api_key=groq_api_key)
        response = client.audio.speech.create(model=model_name, voice=voice, input=text_to_speak, response_format="wav")
        output_path = os.path.join(output_dir, output_filename)
        response.write_to_file(output_path)
        print(f"   [+] Saved Groq TTS output to: {output_path}")
    except Exception as e: print(f"   [!] Error (Groq TTS): {e}")

# ==============================================================================
# SECTION 4: SPEECH-TO-TEXT (STT)
# ==============================================================================
# --- OpenAI STT (Whisper) ---
def call_openai_stt(audio_filepath: str, model_name: str = "whisper-1"):
    print(f"\n--- Calling OpenAI STT (Whisper) (Model: {model_name}) ---")
    if openai_api_key.startswith("YOUR_"): print("   [!] OpenAI API key not set. Skipping."); return
    if not os.path.exists(audio_filepath): print(f"   [!] Audio file not found: {audio_filepath}. Skipping."); return
    try:
        client = OpenAIClient(api_key=openai_api_key)
        with open(audio_filepath, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(model=model_name, file=audio_file)
            print("   [->] Transcription:", transcript.text)
    except Exception as e: print(f"   [!] Error (OpenAI STT): {e}")

# --- Google Cloud STT ---
def call_google_cloud_stt(audio_filepath: str, language_code: str = "en-US"):
    print(f"\n--- Calling Google Cloud STT ---")
    if gcp_project_id == "your-gcp-project-id": print("   [!] GCP Project ID not set. Skipping."); return
    if not os.path.exists(audio_filepath): print(f"   [!] Audio file not found: {audio_filepath}. Skipping."); return
    try:
        client = google_stt.SpeechClient()
        with io.open(audio_filepath, "rb") as audio_file: content = audio_file.read()
        audio = google_stt.RecognitionAudio(content=content)
        config = google_stt.RecognitionConfig(language_code=language_code)
        if audio_filepath.lower().endswith(".wav"): config.encoding = google_stt.RecognitionConfig.AudioEncoding.LINEAR16
        elif audio_filepath.lower().endswith(".mp3"): config.encoding = google_stt.RecognitionConfig.AudioEncoding.MP3
        else: config.encoding = google_stt.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED
        print("   [i] Sending audio to Google Cloud STT API...")
        response = client.recognize(config=config, audio=audio)
        print("   [i] Processing response...")
        if response.results:
            print("   [->] Transcription:")
            for result in response.results: print(f"      - {result.alternatives[0].transcript} (Conf: {result.alternatives[0].confidence:.2f})")
        else: print("   [!] Google Cloud STT returned no results.")
    except Exception as e: print(f"   [!] Error (Google Cloud STT): {e}. Check GCP auth, API, audio format/config.")

# --- Groq STT (Whisper models) ---
def call_groq_stt(audio_filepath: str, model_name: str = "whisper-large-v3-turbo", language: str = "en"):
    print(f"\n--- Calling Groq STT (Model: {model_name}) ---")
    if groq_api_key.startswith("YOUR_"): print("   [!] Groq API key not set. Skipping."); return
    if not os.path.exists(audio_filepath): print(f"   [!] Audio file not found: {audio_filepath}. Skipping."); return
    try:
        client = GroqClient(api_key=groq_api_key)
        with open(audio_filepath, "rb") as file:
            transcription = client.audio.transcriptions.create(
              file=(os.path.basename(audio_filepath), file.read()), model=model_name,
              response_format="verbose_json", language=language, timestamp_granularities = ["word", "segment"]
            )
            print("   [->] Transcription (verbose_json):")
            print(json.dumps(transcription.to_dict(), indent=2, default=str))
            if hasattr(transcription, 'text'): print("\n   [->] Transcription Text Only:", transcription.text)
    except Exception as e: print(f"   [!] Error (Groq STT): {e}")

# ==============================================================================
# --- Main Execution Example ---
# ==============================================================================
if __name__ == "__main__":

    # --- Create Dummy Files if they don't exist ---
    # Dummy Audio for STT
    if not os.path.exists(sample_audio_path_for_stt):
        print(f"\n[!] Sample audio file '{sample_audio_path_for_stt}' not found. Creating dummy silent WAV.")
        try:
            import wave, struct; sr, dur, bits, nchan = 16000, 1, 16, 1; bps, blk_align = bits // 8, nchan * (bits // 8); num_samp = dur * sr
            with wave.open(sample_audio_path_for_stt, 'wb') as wf:
                 wf.setnchannels(nchan); wf.setsampwidth(bps); wf.setframerate(sr); wf.setnframes(num_samp); wf.setcomptype('NONE', 'not compressed')
                 for _ in range(num_samp): wf.writeframesraw(struct.pack('<h', 0))
            print(f"    Dummy WAV file created: {sample_audio_path_for_stt}")
        except Exception as e: print(f"    [!] Failed to create dummy WAV: {e}. STT examples may fail."); sample_audio_path_for_stt = None
    # Dummy Image for Vision
    if not os.path.exists(sample_image_path_for_vision):
         print(f"\n[!] Sample image file '{sample_image_path_for_vision}' not found. Creating dummy black JPEG.")
         try: Image.new('RGB', (60, 30), color = 'black').save(sample_image_path_for_vision); print(f"    Dummy JPEG file created: {sample_image_path_for_vision}")
         except Exception as e: print(f"    [!] Failed to create dummy image: {e}. Vision examples using local path may fail."); sample_image_path_for_vision = None

    # --- Example Calls (Uncomment the ones you want to run) ---

    print("\n" + "="*30 + " SECTION 1: Chat (Text & Vision) " + "="*30)
    chat_prompt = "Explain the concept of AI inference in simple terms, focusing on the difference from training."
    vision_prompt = "Describe this image in detail."

    # --- Text Only ---
    # call_openai_chat(chat_prompt)
    # call_anthropic_chat(chat_prompt)
    # call_vertex_ai_chat(chat_prompt) # Needs GCP setup
    # call_google_ai_chat(chat_prompt) # Needs Google AI Key
    # call_mistral_chat(chat_prompt)
    # call_groq_chat(chat_prompt, model_name="llama3-70b-8192") # Example using a text model on Groq
    # call_lm_studio_chat(chat_prompt) # Needs LM Studio Server running
    # call_ollama_chat(chat_prompt, model_name="llama3") # Needs Ollama running + model pulled

    # --- Vision Enabled ---
    if sample_image_path_for_vision:
         print("\n--- Vision Examples (Using Local Image) ---")
         # call_openai_chat(vision_prompt, image_path=sample_image_path_for_vision, model_name="gpt-4o")
         # call_anthropic_chat(vision_prompt, image_path=sample_image_path_for_vision) # Claude 3 models
         # call_vertex_ai_chat(vision_prompt, image_path=sample_image_path_for_vision, model_name="gemini-1.5-pro-preview-0409") # Needs GCP & vision model
         # call_google_ai_chat(vision_prompt, image_path=sample_image_path_for_vision, model_name="gemini-1.5-pro") # Needs Google AI Key & vision model
         # call_groq_chat(vision_prompt, image_path=sample_image_path_for_vision, model_name="llama-3.2-11b-vision-preview") # Use Groq vision model
         pass # Uncomment calls above
    else: print("\n[!] Skipping vision examples using local path as sample image is missing.")

    # --- Vision Enabled (using image URL) ---
    # print("\n--- Vision Examples (Using Image URL) ---")
    # call_openai_chat(vision_prompt, image_url=sample_image_url_for_vision, model_name="gpt-4o")
    # call_vertex_ai_chat(vision_prompt, image_gcs_uri=sample_image_url_for_vision, model_name="gemini-1.5-pro-preview-0409") # Needs public GCS URI or similar
    # call_groq_chat(vision_prompt, image_url=sample_image_url_for_vision, model_name="llama-3.2-11b-vision-preview") # Use Groq vision model with URL


    print("\n" + "="*30 + " SECTION 2: Image Generation " + "="*30)
    image_gen_prompt = "A vibrant coral reef teeming with colorful fish, digital art style"
    # call_openai_image_gen(image_gen_prompt, n=1)
    # call_vertex_ai_image_gen(image_gen_prompt, n=1) # Needs GCP setup
    # call_stability_ai_image_gen(image_gen_prompt, samples=1) # Needs Stability Key


    print("\n" + "="*30 + " SECTION 3: Text-to-Speech (TTS) " + "="*30)
    tts_prompt = "Groq now offers incredibly fast text to speech synthesis."
    # call_openai_tts(tts_prompt)
    # call_google_cloud_tts(tts_prompt) # Needs GCP setup
    # call_groq_tts(tts_prompt) # Example using Groq TTS


    print("\n" + "="*30 + " SECTION 4: Speech-to-Text (STT) " + "="*30)
    if sample_audio_path_for_stt:
        print(f"\n--- STT Examples (Using Audio File: {sample_audio_path_for_stt}) ---")
        # call_openai_stt(sample_audio_path_for_stt)
        # call_google_cloud_stt(sample_audio_path_for_stt) # Needs GCP setup + potentially better config based on file
        # call_groq_stt(sample_audio_path_for_stt, model_name="whisper-large-v3-turbo") # Example using Groq STT
        pass # Uncomment calls above
    else:
        print("\n[!] Skipping STT examples because sample audio file is missing or failed to create.")


    print("\n--- All API calls attempted (check logs for successes/failures/skips). ---")
    print(f"--- Outputs (like audio/images) saved in: '{output_dir}' ---")


# ==============================================================================
# SUGGESTED IMPROVEMENTS & FUTURE WORK
# ==============================================================================
#
# This script provides a functional overview, but here are areas for enhancement:
#
# 1.  **Robust Error Handling:** Implement more specific `try...except` blocks for
#     API errors (e.g., rate limits, authentication issues, invalid requests) and
#     network errors. Add retry mechanisms (e.g., using libraries like `tenacity`).
# 2.  **Configuration Management:** Use a more structured approach for configuration
#     (API keys, model names, file paths) like `.env` files (`python-dotenv`),
#     YAML/JSON config files, or dedicated config libraries.
# 3.  **Asynchronous Operations:** For applications requiring higher concurrency,
#     explore and add examples using asynchronous clients (`asyncio` with `aiohttp`
#     for REST APIs or async versions of SDKs where available).
# 4.  **Streaming Responses:** Add examples demonstrating how to handle streaming
#     responses, particularly for Chat/LLM calls and potentially STT, for more
#     interactive applications.
# 5.  **Advanced Parameters:** Include examples showcasing more advanced API parameters
#     for finer control (e.g., `top_p`, `frequency_penalty`, `logit_bias` for LLMs;
#     negative prompts, styles, aspect ratios for image gen; diarization for STT).
# 6.  **Modularity:** Refactor the code into classes (e.g., `OpenAIHandler`,
#     `GroqHandler`) to improve organization, maintainability, and reusability,
#     especially if integrating into larger projects.
# 7.  **Input Validation:** Add more explicit checks for input validity (e.g.,
#     verifying audio file formats/encodings before sending to STT APIs, checking
#     image dimensions/types).
# 8.  **Local Multimodality Details:** Provide more concrete examples or links to
#     documentation on how to set up and call multimodal models (like LLaVA)
#     on local servers (Ollama/LM Studio), as it often requires specific request
#     formats beyond the standard chat client usage shown here.
# 9.  **Dependency Management:** Include a `requirements.txt` or integrate with
#     `pyproject.toml` (using Poetry or standard pip) alongside the script in the repo.
# 10. **Cost Estimation/Tracking:** Integrate basic cost estimation based on token
#     counts (where available in responses) and provider pricing.
# 11. **Tool Use / Function Calling:** Add examples for providers that support
#     tool use or function calling (OpenAI, Anthropic, Google, Groq).
# 12. **Batch Processing:** Include examples for batch APIs where offered (e.g., OpenAI Batch API).
# 13. **More Providers:** Expand coverage to include other relevant platforms like
#     AWS Bedrock, Azure OpenAI Service, Cohere (direct API), Replicate, etc.
# 14. **Testing:** Add unit or integration tests to verify API interactions.
#
# ==============================================================================
